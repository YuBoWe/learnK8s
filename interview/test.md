## 1. 你的 Job 挂载 NFS 存储，如果多个计算任务同时写一个文件，你打算怎么做并发控制 
数据一致性与资源竞争。在隐私计算（如多方安全计算或联邦学习）中，如果多个 Job 同时往 NFS 写入模型权重或日志，极易导致文件损坏。
从 K8s 编排和分布式锁两个层面来回答
核心回答逻辑
1. 架构层面的规避：唯一性子目录（首选方案）

最简单也最稳健的方法是避免竞争。 “在我的 Job 模板中，我不会让所有 Job 写同一个文件。我会利用 Job 的 pod-template-hash 或者在 Entrypoint 脚本中获取 Pod 的名称/UID，为每个 Job 在 NFS 挂载点下创建一个独立的子目录。”

    优点： 零冲突，天然支持高并发。

2. 系统层面的控制：利用 NFS 的文件锁 (flock)

如果你必须操作同一个文件，就要提到 Linux 内核特性。 “由于 NFS 支持 NLM (Network Lock Manager) 协议，我可以在 Python 或 C++ 编写的计算脚本中使用 fcntl 或 flock 对目标文件加排他锁。当一个 Job 写入时，其他 Job 会在应用层挂起等待。”

    注意点： 要提醒导师，NFSv3 的锁较弱，建议使用 NFSv4，它对文件锁的支持更健壮。

3. K8s 高级控制：分布式锁服务 (Redis/Etcd)

这是展示你高阶能力的时刻。 “对于复杂的并发场景，我会引入一个轻量级的 Redis 或直接利用 K8s 自带的 Etcd 作为分布式锁中心。

    Job 写入前，先去 Redis 申请一个以文件名为 Key 的锁（设置过期时间防止死锁）。

    写入完成后释放锁。”

4. 业务逻辑层面：生产者-消费者模型

“我还可以改变写入方式。让多个计算 Job 只负责生成结果并写入各自的临时文件，然后由一个 CronJob 或者 StatefulSet 运行的‘聚合器’，定期扫描 NFS 目录并将结果汇总到主文件中。”

    价值： 这种异步处理非常符合联邦学习中的“参数聚合”逻辑。

导师追问：如果 Job 拿到锁后突然宕机了（Crash），导致锁没释放，后面的 Job 全卡死了怎么办？
> 你的反击： “这就是为什么我倾向于使用 Redis 结合 TTL（生存时间）。我会给锁设置一个略大于计算时长的过期时间。 另外，在 K8s 中，我可以利用 Job 的 activeDeadlineSeconds 属性，强制终止超时任务，并在 Pod 的 PreStop 钩子里加入清理脚本，确保极端情况下锁资源的回收。

## 2.  你说你用了 OpenELB，那你告诉我，在你的 5 节点裸金属环境下，你是怎么解决 LoadBalancer 的 IP 地址漂移问题的？
围绕 “二层网络（Layer 2）” 和 “ARP/隐式通告” 展开
核心标准回答

你可以分三个层次来回答，层层递进，展示你的工程深度：
第一层：明确模式（展示选型能力）

“在我的 5 节点裸金属集群中，我配置 OpenELB 使用了 Layer2 模式。因为在没有硬件交换机支持 BGP 协议的情况下，这是在私有网络实现 LoadBalancer 最稳定、成本最低的方案。”
第二层：技术原理（展示底层功底）

“为了解决 IP 地址漂移和高可用问题，我利用了 OpenELB 的 Speaker 机制。

    当一个 Service 创建时，OpenELB 会在集群中选出一个节点作为 Announcer（通告者）。

    这个节点会通过发送 免费 ARP（Gratuitous ARP），告诉网关和局域网内的其他设备：‘LoadBalancer 的这个 VIP 现在在我这台物理机的 MAC 地址上’。

    这样，外部流量就会精准地流向这个特定的物理节点。”

第三层：故障转移（核心：解决漂移后的流量接管）

“至于地址漂移，这正是 OpenELB 的高可用特性：

    我配置了 健康检查机制。如果当前承载 VIP 的节点发生故障（宕机或网络断开），OpenELB 的控制平面会迅速感知。

    它会立即在剩下的健康节点中重新选主，新的 Speaker 节点会再次发送 免费 ARP，强制刷新局域网内交换机或路由器的 ARP 表项。

    这样 VIP 就‘漂移’到了新节点，整个过程秒级完成，保证了服务不中断。”

